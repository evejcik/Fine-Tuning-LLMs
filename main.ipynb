{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU not detected. Check your CUDA installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler, TrainingArguments, Trainer\n",
    "from huggingface_hub.inference_api import InferenceApi\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hugging face access token: \n",
    "# API_TOKEN = \"\"\n",
    "# os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "#huggingface-cli login ****Edit -> paste**** (not CTRL+V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "inference = InferenceApi(repo_id=\"bert-base-uncased\", token=API_TOKEN)\n",
    "# response = inference(inputs=\"The goal of life is [MASK].\", raw_response =True)\n",
    "# print(response.json())\n",
    "# Load the Gemini tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  __label__2 Great CD: My lovely Pat has one of ...\n",
      "1  __label__2 One of the best game music soundtra...\n",
      "2  __label__1 Batteries died within a year ...: I...\n",
      "3  __label__2 works fine, but Maha Energy is bett...\n",
      "4  __label__2 Great for the non-audiophile: Revie...\n",
      "                                                text\n",
      "0  __label__2 Stuning even for the non-gamer: Thi...\n",
      "1  __label__2 The best soundtrack ever to anythin...\n",
      "2  __label__2 Amazing!: This soundtrack is my fav...\n",
      "3  __label__2 Excellent Soundtrack: I truly like ...\n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with bz2.open(\"test.ft.txt.bz2/test.ft.txt.bz2\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f]  # Remove newlines\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(lines, columns=[\"text\"])\n",
    "\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "with bz2.open(\"train.ft.txt.bz2/train.ft.txt.bz2\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f]  # Remove newlines\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(lines, columns=[\"text\"])\n",
    "\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_np = df_test.to_numpy().reshape(-1,1)\n",
    "df_test_np = df_test_np[:300]\n",
    "#for row in df_np: extract first 10 letters and make it new column as label column\n",
    "df_test_np = np.hstack((np.array([row[:10] for row in df_test_np[:, 0]]).reshape(-1,1),np.array([row[10:] for row in df_test_np[:,0]]).reshape(-1,1)))\n",
    "\n",
    "df_test_pd = pd.DataFrame(df_test_np, columns=[\"label\",\"text\"])\n",
    "\n",
    "df_train_np = df_train.to_numpy().reshape(-1,1)\n",
    "df_train_np = df_train_np[:600]\n",
    "#for row in df_np: extract first 10 letters and make it new column as label column\n",
    "df_train_np = np.hstack((np.array([row[:10] for row in df_train_np[:, 0]]).reshape(-1,1),np.array([row[10:] for row in df_train_np[:,0]]).reshape(-1,1)))\n",
    "df_train_pd = pd.DataFrame(df_train_np, columns=[\"label\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df, num_samples, classes_to_keep):\n",
    "    # Sample rows, selecting num_samples of each Label.                                      \n",
    "    df = (\n",
    "        df.groupby(\"Label\")[df.columns]\n",
    "        .apply(lambda x: x.sample(num_samples))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb5e443b0b241fa8ae17aef064b0e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512) #512 helps keep memory cost down for GPU\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_dataset['label'][:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "test_valid_split = split_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "tokenized_dataset_dict = {\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test']\n",
    "}\n",
    "\n",
    "# print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# print(\"Version:\", torchvision.__version__)\n",
    "# print(\"Location:\", torchvision.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at describeai/gemini and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"describeai/gemini\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"s\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 400,\n",
      "      \"num_beams\": 2,\n",
      "      \"prefix\": \"s:\"\n",
      "    },\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Gemini model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"describeai/gemini\", num_labels=2) #2 labels since only doing binary classification here, match to num labels for multi-class classification\n",
    "\n",
    "# Customize the configuration\n",
    "model.config.hidden_dropout_prob = 0.1  # Reduce overfitting\n",
    "model.config.attention_probs_dropout_prob = 0.1\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ClassificationHead(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_head.dense.weight torch.Size([1024, 1024])\n",
      "classification_head.dense.bias torch.Size([1024])\n",
      "classification_head.out_proj.weight torch.Size([2, 1024])\n",
      "classification_head.out_proj.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"classification_head\" in name:\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['transformer.encoder.block.22.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.22.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.22.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.22.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.22.layer.0.layer_norm.weight', 'transformer.encoder.block.22.layer.1.DenseReluDense.wi.weight', 'transformer.encoder.block.22.layer.1.DenseReluDense.wo.weight', 'transformer.encoder.block.22.layer.1.layer_norm.weight', 'transformer.encoder.block.23.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.23.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.23.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.23.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.23.layer.0.layer_norm.weight', 'transformer.encoder.block.23.layer.1.DenseReluDense.wi.weight', 'transformer.encoder.block.23.layer.1.DenseReluDense.wo.weight', 'transformer.encoder.block.23.layer.1.layer_norm.weight', 'classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n"
     ]
    }
   ],
   "source": [
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification head (if needed)\n",
    "for name, param in model.classification_head.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last 2 encoder blocks (example: if the encoder blocks are named \"encoder.block.X\")\n",
    "# Adjust the key name pattern based on your model's naming convention\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.block\" in name:\n",
    "        # Extract block number if available (this depends on the naming convention)\n",
    "        block_number = int(name.split(\"encoder.block.\")[1].split(\".\")[0])\n",
    "        if block_number >= (model.config.num_layers - 2):  # unfreeze the last 2 blocks\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Check which parameters will be updated\n",
    "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(\"Trainable parameters:\", trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "train_dataloader = DataLoader(split_dataset['train'], shuffle=True, batch_size=16)\n",
    "num_epochs = 3\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655ae90d20854ea8beab53d099fe3770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420f0549470041b29163e32b472ac70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda5eb8d8ae94d3194789114d2b5fd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "test_valid_split = split_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "tokenized_dataset_dict = {\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test']\n",
    "}\n",
    "\n",
    "def convert_labels(example):\n",
    "    # Adjust this mapping based on your actual label names\n",
    "    mapping = {\"__label__2\": 0, \"__label__1\": 1}\n",
    "    example[\"label\"] = mapping[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "tokenized_dataset_dict[\"train\"] = tokenized_dataset_dict[\"train\"].map(convert_labels)\n",
    "tokenized_dataset_dict[\"validation\"] = tokenized_dataset_dict[\"validation\"].map(convert_labels)\n",
    "tokenized_dataset_dict[\"test\"] = tokenized_dataset_dict[\"test\"].map(convert_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",              # Save directory\n",
    "#     eval_strategy=\"epoch\",        # Evaluate after every epoch\n",
    "#     learning_rate=2e-5,                 # Initial learning rate\n",
    "#     per_device_train_batch_size=4,     # Batch size per device\n",
    "#     num_train_epochs=3,                 # Number of epochs\n",
    "#     weight_decay=0.01,                  # Regularization\n",
    "#     logging_dir=\"./logs\",               # Log directory\n",
    "#     save_total_limit=2,                 # Save only the last 2 checkpoints\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # Your model\n",
    "#     args=training_args,                  # Training arguments\n",
    "#     train_dataset=tokenized_dataset_dict['train'],  # Training data\n",
    "#     eval_dataset=tokenized_dataset_dict['validation'],  # Validation data\n",
    "# )\n",
    "\n",
    "# # Start fine-tuning\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated on current device: 0\n",
      "Memory reserved on current device: 0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Memory allocated on current device:\", torch.cuda.memory_allocated())\n",
    "print(\"Memory reserved on current device:\", torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 5:02:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.717282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.693261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.690792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.683642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>0.684051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.6797167555491129, metrics={'train_runtime': 18249.2883, 'train_samples_per_second': 0.132, 'train_steps_per_second': 0.008, 'total_flos': 5203867115520000.0, 'train_loss': 0.6797167555491129, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True, max_split_size_mb:128\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,         # smaller batch size\n",
    "    gradient_accumulation_steps=2,           # accumulate gradients to simulate a batch size of 4\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps = 10,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=4                             # enable mixed precision training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_dict['train'],\n",
    "    eval_dataset=tokenized_dataset_dict['validation'],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
