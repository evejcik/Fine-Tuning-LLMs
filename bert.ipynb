{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not detected. Check your CUDA installation.\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler, TrainingArguments, Trainer, EarlyStoppingCallback, TrainerCallback, pipeline, set_seed, BertModel\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from huggingface_hub.inference_api import InferenceApi\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import sqlite3 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import optuna\n",
    "\n",
    "\n",
    "from customhead import CustomClassificationHead\n",
    "import tensorboardX\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_TOKEN = \n",
    "# os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in dataset: [1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#data loading block\n",
    "\n",
    "df = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "def categorize_rating(rating):\n",
    "    if rating <= 3:\n",
    "        return 0  # Negative\n",
    "    elif 4 <= rating <= 6:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "    \n",
    "df['labels'] = df['RATING'].apply(categorize_rating)\n",
    "\n",
    "\n",
    "#  Debug: Check label distribution\n",
    "print(\"Unique labels in dataset:\", df[\"labels\"].unique())  # Should output [0,1,2]\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)  # If labels are in range [0-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bert_layers(model, freeze_percent=50):\n",
    "    \"\"\"\n",
    "    Freezes the first 'freeze_percent' of BERT layers while leaving the rest trainable.\n",
    "    \n",
    "    Args:\n",
    "    - model: Pretrained BERT model.\n",
    "    - freeze_percent: Percentage of layers to freeze (0-100).\n",
    "    \"\"\"\n",
    "    total_layers = len(model.bert.encoder.layer)  # Total transformer layers (12 for BERT-base)\n",
    "    num_freeze = int((freeze_percent / 100) * total_layers)  # Number of layers to freeze\n",
    "\n",
    "    # Freeze embeddings layer (always)\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Freeze the first 'num_freeze' layers\n",
    "    for layer in model.bert.encoder.layer[:num_freeze]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    print(f\"Frozen {num_freeze}/{total_layers} encoder layers ({freeze_percent}%).\")\n",
    "\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = list(texts)  \n",
    "        self.labels = [int(label) for label in labels]  # Ensure labels are integers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Debugging: Print first few samples\n",
    "        print(\"[DEBUG] First 3 items in dataset:\")\n",
    "        for i in range(min(3, len(self.texts))):\n",
    "            print(f\"Text {i}: {self.texts[i]} | Label: {self.labels[i]} | Type: {type(self.labels[i])}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Debugging\n",
    "        if idx < 5:\n",
    "            print(f\"[DEBUG] Fetching index {idx}: Text={text}, Label={label}, Type={type(label)}\")\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            encoding[\"input_ids\"].squeeze(0),\n",
    "            encoding[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long)  # Ensure it's a `long` tensor\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using Only BERT for Classification: fine tunes the entire BERT model directly, with varying percentages of frozen layers, trained end to end using our dataset. This model already includes a classification head (a linear layer on top of BERTâ€™s [CLS] token representation), and you fine-tune the entire model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in dataset: [1 2 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] First 3 items in dataset:\n",
      "Text 0: I was expecting this movie to be bad, but in a good way, if you get me. But, boy, this is an embarassement. No fun, no real action. The thing is painfull. I d give it a ZERO if possible. Worst movie ever. | Label: 0 | Type: <class 'int'>\n",
      "Text 1: Father Peter (Guy Pearce ) is an exorcist who smokes and swears. He drives a car that needs a muffler. Peter was trained by the late Father Louis (Keith David) who died on his last exorcism as well as the boy who was possessed. He is assigned Father Daniel as an intern exorcist who is immediately thrust into the lion's den. They are working on a kid named Charley (pencils are involved later) who once played with a Ouija Board at a skating rink.The film has a twist which Charley gives away. It was okay if you have never seen an exorcist film before.Guide: F-word. No sex or nudity. | Label: 1 | Type: <class 'int'>\n",
      "Text 2: I never write reviews. This movie was so bad I had to say something. This year the academy awards had a 58% viewer rating because of woke Hollywood. You have lost our trust. Don't see this movie, don't say her name. Go outside and play with your kids instead. | Label: 0 | Type: <class 'int'>\n",
      "[DEBUG] First 3 items in dataset:\n",
      "Text 0: Malcolm & Marie gives the opportunity to two young actors to shine. Zendaya impresses us with an outstanding performance as well as John David Washington. They both show us how they can turn a not pretentious movie with a plain plot into a mind blowing film. Rating: 8,5 | Label: 2 | Type: <class 'int'>\n",
      "Text 1: What has become of you Nicolas?? You used to be in such awesome movies, but now you're fighting animatronic monkeys with a toilet plunger and slapping animatronic alligators... Such a waste of talent... This movie is truely awful. | Label: 0 | Type: <class 'int'>\n",
      "Text 2: Why? Seriously, why? Does she have something on someone important? Does she know someone that can green light anything? Did she sign a blood contract with Satan, I mean, seriously, why? She had a few episodes on Orange is the new black, and then major roles. She is getting roles that an actor should have be getting if they were in the business for like 20+ years, so why? Being serious here... | Label: 0 | Type: <class 'int'>\n",
      "\n",
      "ðŸ”¹ Training with 0% of BERT layers frozen.\n",
      "âœ… Frozen 0/12 encoder layers (0%).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Fetching index 1: Text=Father Peter (Guy Pearce ) is an exorcist who smokes and swears. He drives a car that needs a muffler. Peter was trained by the late Father Louis (Keith David) who died on his last exorcism as well as the boy who was possessed. He is assigned Father Daniel as an intern exorcist who is immediately thrust into the lion's den. They are working on a kid named Charley (pencils are involved later) who once played with a Ouija Board at a skating rink.The film has a twist which Charley gives away. It was okay if you have never seen an exorcist film before.Guide: F-word. No sex or nudity., Label=1, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 3: Text=It's not a fast paced action film, it's a beautifully slow, gorgeous film that covers a real-life story. For anyone who likes history, or archaeology, this is absolutely fascinating. It's so nice to see films like this these days when most are action or thriller., Label=2, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 0: Text=I was expecting this movie to be bad, but in a good way, if you get me. But, boy, this is an embarassement. No fun, no real action. The thing is painfull. I d give it a ZERO if possible. Worst movie ever., Label=0, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 4: Text=This movie has 0 credibility and no integrity. It does high dishonor to not only Navy SEALs but to the United States military. Nothing made sense. It was like a bunch of lazy betas and feminist wrote, directed and produced this trash.High disrespect to Tom Clancy and his legacy., Label=0, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 2: Text=I never write reviews. This movie was so bad I had to say something. This year the academy awards had a 58% viewer rating because of woke Hollywood. You have lost our trust. Don't see this movie, don't say her name. Go outside and play with your kids instead., Label=0, Type=<class 'int'>\n",
      "Epoch 1/10, Loss: 0.8984\n",
      "[DEBUG] Fetching index 0: Text=Malcolm & Marie gives the opportunity to two young actors to shine. Zendaya impresses us with an outstanding performance as well as John David Washington. They both show us how they can turn a not pretentious movie with a plain plot into a mind blowing film. Rating: 8,5, Label=2, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 1: Text=What has become of you Nicolas?? You used to be in such awesome movies, but now you're fighting animatronic monkeys with a toilet plunger and slapping animatronic alligators... Such a waste of talent... This movie is truely awful., Label=0, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 2: Text=Why? Seriously, why? Does she have something on someone important? Does she know someone that can green light anything? Did she sign a blood contract with Satan, I mean, seriously, why? She had a few episodes on Orange is the new black, and then major roles. She is getting roles that an actor should have be getting if they were in the business for like 20+ years, so why? Being serious here..., Label=0, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 3: Text=We need more movies with the same message.\n",
      "Unfortunately, if you assume your audience is dumb you end up looking dumb. The message is so thick that any original plot line or characters drowns., Label=0, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 4: Text=The only this i can say that this is a Garbage , No brain used, betrayal for no reason, mission useless , everything wrong with this movie., Label=0, Type=<class 'int'>\n",
      "Test Accuracy: 0.7760\n",
      "[DEBUG] Fetching index 3: Text=It's not a fast paced action film, it's a beautifully slow, gorgeous film that covers a real-life story. For anyone who likes history, or archaeology, this is absolutely fascinating. It's so nice to see films like this these days when most are action or thriller., Label=2, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 4: Text=This movie has 0 credibility and no integrity. It does high dishonor to not only Navy SEALs but to the United States military. Nothing made sense. It was like a bunch of lazy betas and feminist wrote, directed and produced this trash.High disrespect to Tom Clancy and his legacy., Label=0, Type=<class 'int'>\n",
      "[DEBUG] Fetching index 1: Text=Father Peter (Guy Pearce ) is an exorcist who smokes and swears. He drives a car that needs a muffler. Peter was trained by the late Father Louis (Keith David) who died on his last exorcism as well as the boy who was possessed. He is assigned Father Daniel as an intern exorcist who is immediately thrust into the lion's den. They are working on a kid named Charley (pencils are involved later) who once played with a Ouija Board at a skating rink.The film has a twist which Charley gives away. It was okay if you have never seen an exorcist film before.Guide: F-word. No sex or nudity., Label=1, Type=<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# âœ… Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# âœ… Load dataset and drop missing values\n",
    "df = pd.read_csv(\"movies.csv\").dropna()\n",
    "\n",
    "# âœ… Convert RATING into 3-Class Labels\n",
    "def categorize_rating(rating):\n",
    "    if rating <= 3:\n",
    "        return 0  # Negative\n",
    "    elif 4 <= rating <= 6:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "df['labels'] = df['RATING'].apply(categorize_rating)\n",
    "\n",
    "# âœ… Debug: Check label distribution\n",
    "print(\"Unique labels in dataset:\", df[\"labels\"].unique())  # Should output [0,1,2]\n",
    "\n",
    "# âœ… Split dataset into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['REVIEW'].tolist(), df['labels'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# âœ… Define Dataset Class\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = list(texts)  \n",
    "        self.labels = [int(label) for label in labels]  # ðŸ”¥ Ensure labels are integers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # âœ… Debugging: Print first few samples\n",
    "        print(\"[DEBUG] First 3 items in dataset:\")\n",
    "        for i in range(min(3, len(self.texts))):\n",
    "            print(f\"Text {i}: {self.texts[i]} | Label: {self.labels[i]} | Type: {type(self.labels[i])}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # âœ… Debugging\n",
    "        if idx < 5:\n",
    "            print(f\"[DEBUG] Fetching index {idx}: Text={text}, Label={label}, Type={type(label)}\")\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            encoding[\"input_ids\"].squeeze(0),\n",
    "            encoding[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long)  # ðŸ”¥ Ensure it's a `long` tensor\n",
    "        )\n",
    "\n",
    "# âœ… Create Dataset Instances\n",
    "train_dataset = IMDbDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = IMDbDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# âœ… Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# âœ… Function to Freeze BERT Layers\n",
    "def freeze_bert_layers(model, freeze_percent=50):\n",
    "    total_layers = len(model.bert.encoder.layer)  # Total transformer layers (12 for BERT-base)\n",
    "    num_freeze = int((freeze_percent / 100) * total_layers)  # Number of layers to freeze\n",
    "\n",
    "    # ðŸ”’ Always freeze embeddings layer\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # ðŸ”’ Freeze first 'num_freeze' layers\n",
    "    for layer in model.bert.encoder.layer[:num_freeze]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    print(f\"âœ… Frozen {num_freeze}/{total_layers} encoder layers ({freeze_percent}%).\")\n",
    "\n",
    "# âœ… Freezing Percentages to Test\n",
    "freeze_pcts = [0, 25, 50, 75, 100]\n",
    "results = []  # Store results for comparison\n",
    "\n",
    "# âœ… Loop Over Freezing Percentages\n",
    "for pct in freeze_pcts:\n",
    "    print(f\"\\nðŸ”¹ Training with {pct}% of BERT layers frozen.\")\n",
    "\n",
    "    # âœ… Load Fresh BERT Model for Each Experiment\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", \n",
    "        num_labels=3,  # âœ… Match it with 3-class classification\n",
    "        hidden_dropout_prob=0.3,  \n",
    "        attention_probs_dropout_prob=0.3\n",
    "    ).to(device)\n",
    "\n",
    "    # âœ… Freeze Layers Based on Percentage\n",
    "    freeze_bert_layers(model, freeze_percent=pct)\n",
    "\n",
    "    # âœ… Define Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "    # âœ… Compute Class Weights Only If Using 3-Class Labels\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    num_epochs = 10\n",
    "    best_accuracy = 0\n",
    "    patience = 3  # Stop training if accuracy doesn't improve for 3 epochs\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # âœ… Evaluate Every Epoch\n",
    "        model.eval()\n",
    "        predictions, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in test_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # âœ… Early Stopping: Save Best Model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_model_{pct}.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break  \n",
    "\n",
    "    # âœ… Store Results for Comparison\n",
    "    results.append((pct, best_accuracy))\n",
    "\n",
    "# âœ… Compare Results\n",
    "print(\"\\nðŸ“ˆ Final Results:\")\n",
    "for pct, acc in results:\n",
    "    print(f\"Frozen {pct}% Layers â†’ Accuracy: {acc:.4f}\")\n",
    "\n",
    "# âœ… Plot Accuracy vs. Frozen Layers\n",
    "freeze_pcts, accuracies = zip(*results)\n",
    "\n",
    "plt.plot(freeze_pcts, accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Frozen Layers (%)\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Impact of Freezing BERT Layers on Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use BERT to Encode Reviews and Use an MLP on Top. Here, BERT is used as a feature extractor. Instead of training it end-to-end, we extract embeddings and pass them to an MLP for classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a pre-trained BERT model (without its classification head).\n",
    "- Extract [CLS] token representations as features.\n",
    "- Pass these embeddings to an MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)  # Second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def extract_features(text_list, tokenizer, model, batch_size=4):  # Reduce batch size\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            batch_features = outputs.last_hidden_state[:, 0, :].to(device)\n",
    "            features.append(batch_features)\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"[ERROR] CUDA OOM at batch {i//batch_size + 1}, trying smaller batch size...\")\n",
    "                return None  # Return None so you can handle failure outside\n",
    "\n",
    "            else:\n",
    "                raise e  # Re-raise any other error\n",
    "\n",
    "    return torch.cat(features, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before model init:\n",
      "Memory allocated: 0\n",
      "Memory reserved: 0\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Before model init:\")\n",
    "print(\"Memory allocated:\", torch.cuda.memory_allocated())\n",
    "print(\"Memory reserved:\", torch.cuda.memory_reserved())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 0% of BERT layers frozen.\n",
      "Frozen 0/12 encoder layers (0%).\n",
      "Epoch 1, Loss: 1.1053599119186401\n",
      "Test Accuracy: 0.3796\n",
      "Epoch 2, Loss: 1.1046231985092163\n",
      "Test Accuracy: 0.3805\n",
      "Epoch 3, Loss: 1.1038827896118164\n",
      "Test Accuracy: 0.3824\n",
      "Epoch 4, Loss: 1.103144645690918\n",
      "Test Accuracy: 0.3927\n",
      "Epoch 5, Loss: 1.1024106740951538\n",
      "Test Accuracy: 0.3964\n",
      "Epoch 6, Loss: 1.1016817092895508\n",
      "Test Accuracy: 0.3936\n",
      "Epoch 7, Loss: 1.1009575128555298\n",
      "Test Accuracy: 0.3955\n",
      "Epoch 8, Loss: 1.1002378463745117\n",
      "Test Accuracy: 0.3955\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with 25% of BERT layers frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 3/12 encoder layers (25%).\n",
      "Epoch 1, Loss: 1.1165156364440918\n",
      "Test Accuracy: 0.4142\n",
      "Epoch 2, Loss: 1.1152135133743286\n",
      "Test Accuracy: 0.4142\n",
      "Epoch 3, Loss: 1.1139253377914429\n",
      "Test Accuracy: 0.4142\n",
      "Epoch 4, Loss: 1.1126583814620972\n",
      "Test Accuracy: 0.4133\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with 50% of BERT layers frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 6/12 encoder layers (50%).\n",
      "Epoch 1, Loss: 1.1032973527908325\n",
      "Test Accuracy: 0.3768\n",
      "Epoch 2, Loss: 1.1023502349853516\n",
      "Test Accuracy: 0.3758\n",
      "Epoch 3, Loss: 1.1014057397842407\n",
      "Test Accuracy: 0.3758\n",
      "Epoch 4, Loss: 1.1004703044891357\n",
      "Test Accuracy: 0.3777\n",
      "Epoch 5, Loss: 1.0995451211929321\n",
      "Test Accuracy: 0.3768\n",
      "Epoch 6, Loss: 1.098631739616394\n",
      "Test Accuracy: 0.3739\n",
      "Epoch 7, Loss: 1.0977303981781006\n",
      "Test Accuracy: 0.3777\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with 75% of BERT layers frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 9/12 encoder layers (75%).\n",
      "Epoch 1, Loss: 1.118080973625183\n",
      "Test Accuracy: 0.3543\n",
      "Epoch 2, Loss: 1.1169896125793457\n",
      "Test Accuracy: 0.3552\n",
      "Epoch 3, Loss: 1.115904450416565\n",
      "Test Accuracy: 0.3552\n",
      "Epoch 4, Loss: 1.1148324012756348\n",
      "Test Accuracy: 0.3552\n",
      "Epoch 5, Loss: 1.1137744188308716\n",
      "Test Accuracy: 0.3543\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with 100% of BERT layers frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 12/12 encoder layers (100%).\n",
      "Epoch 1, Loss: 1.109156847000122\n",
      "Test Accuracy: 0.2502\n",
      "Epoch 2, Loss: 1.1084381341934204\n",
      "Test Accuracy: 0.2512\n",
      "Epoch 3, Loss: 1.1077128648757935\n",
      "Test Accuracy: 0.2530\n",
      "Epoch 4, Loss: 1.1069871187210083\n",
      "Test Accuracy: 0.2568\n",
      "Epoch 5, Loss: 1.1062629222869873\n",
      "Test Accuracy: 0.2587\n",
      "Epoch 6, Loss: 1.1055408716201782\n",
      "Test Accuracy: 0.2596\n",
      "Epoch 7, Loss: 1.1048216819763184\n",
      "Test Accuracy: 0.2615\n",
      "Epoch 8, Loss: 1.1041057109832764\n",
      "Test Accuracy: 0.2727\n",
      "Epoch 9, Loss: 1.1033928394317627\n",
      "Test Accuracy: 0.2802\n",
      "Epoch 10, Loss: 1.1026840209960938\n",
      "Test Accuracy: 0.2849\n",
      "\n",
      "Final Results: [(0, 0.3964386129334583), (25, 0.41424554826616683), (50, 0.3776944704779756), (75, 0.35520149953139646), (100, 0.2849109653233365)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import collections\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset and drop missing values\n",
    "df = pd.read_csv(\"movies.csv\").dropna()\n",
    "\n",
    "# Convert RATING into 3-Class Labels\n",
    "def categorize_rating(rating):\n",
    "    if rating <= 3:\n",
    "        return 0  # Negative\n",
    "    elif 4 <= rating <= 6:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "df['labels'] = df['RATING'].apply(categorize_rating)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['REVIEW'].tolist(), df['labels'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# label_counts = collections.Counter(train_labels.numpy())\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define MLP Classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to Freeze BERT Layers\n",
    "def freeze_bert_layers(model, freeze_percent=50):\n",
    "    total_layers = len(model.encoder.layer)\n",
    "    num_freeze = int((freeze_percent / 100) * total_layers)\n",
    "\n",
    "    for param in model.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer in model.encoder.layer[:num_freeze]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"Frozen {num_freeze}/{total_layers} encoder layers ({freeze_percent}%).\")\n",
    "\n",
    "# Function to Extract Features from BERT\n",
    "def extract_features(text_list, tokenizer, model, batch_size=4):\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_features = outputs.last_hidden_state[:, 0, :].to(device)\n",
    "        features.append(batch_features)\n",
    "\n",
    "    return torch.cat(features, dim=0)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)  # Load BERT once\n",
    "    train_features = extract_features(train_texts, tokenizer, bert)\n",
    "    test_features = extract_features(test_texts, tokenizer, bert)\n",
    "\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long, device=device)\n",
    "    test_labels = torch.tensor(test_labels, dtype=torch.long, device=device)\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up memory\n",
    "\n",
    "# Freezing Percentages to Test\n",
    "freeze_pcts = [0, 25, 50, 75, 100]\n",
    "results = []  # Store results for comparison\n",
    "\n",
    "# Loop Over Freezing Percentages\n",
    "for pct in freeze_pcts:\n",
    "    print(f\"\\nTraining with {pct}% of BERT layers frozen.\")\n",
    "\n",
    "    # Reload a fresh BERT model for each experiment\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "    freeze_bert_layers(bert, freeze_percent=pct)\n",
    "\n",
    "    # Define a fresh MLP model\n",
    "    mlp = MLPClassifier(input_dim=768, hidden_dim=256, num_labels=3).to(device)\n",
    "\n",
    "    # Define Optimizer\n",
    "    optimizer = AdamW(mlp.parameters(), lr=1e-5)\n",
    "\n",
    "    # Compute Class Weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels.cpu().numpy()), y=train_labels.cpu().numpy())\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 10\n",
    "    best_accuracy = 0\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = mlp(train_features)\n",
    "        loss = loss_fn(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Evaluate Every Epoch\n",
    "        with torch.no_grad():\n",
    "            mlp.eval()\n",
    "            predictions = mlp(test_features)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            accuracy = accuracy_score(test_labels.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            counter = 0\n",
    "            torch.save(mlp.state_dict(), f\"best_model_{pct}.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break  \n",
    "\n",
    "    results.append((pct, best_accuracy))\n",
    "\n",
    "print(\"\\nFinal Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cslabwin\\anaconda3\\envs\\MLPROJECT\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max features\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "X_test_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "# MLP Classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=10, activation=\"relu\", solver=\"adam\", random_state=42)\n",
    "mlp.fit(X_train_tfidf, train_labels.cpu().numpy())\n",
    "\n",
    "# Evaluate\n",
    "test_acc = mlp.score(X_test_tfidf, test_labels.cpu().numpy())\n",
    "print(f\"MLP Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
